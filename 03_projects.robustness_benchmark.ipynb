{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp projects.robustness_benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Robustness Benchmark\n",
    "> Benchmark utility code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from enum import Enum \n",
    "from scp.analysis.binary import performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "Corruption = Enum(\"Corruption\", \n",
    "                  \"gaussian_noise, shot_noise, impulse_noise,\\\n",
    "                   defocus_blur, motion_blur, zoom_blur,\\\n",
    "                   black_corner, char,\\\n",
    "                   brightness_up, brightness_down, contrast, elastic_transform, pixelate, jpeg_compression,\\\n",
    "                   speckle_noise, gaussian_blur, bubble, saturate,\\\n",
    "                   sanity_check\")\n",
    "\n",
    "Perturbation = Enum(\"Perturbation\",\n",
    "                    \"gaussian_noise, shot_noise,\\\n",
    "                     motion_blur, zoom_blur,\\\n",
    "                     char\\\n",
    "                     brightness, translate, rotate, tilt, scale,\\\n",
    "                     speckle_noise, gaussian_blur, bubble, shear,\\\n",
    "                     sanity_check\")\n",
    "\n",
    "max_severity = 6 # severity ranges from 1 to 5 inclusive\n",
    "num_frames = 31 # \n",
    "classes = [\"nevus\", \"melanoma\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "# get balanced error rate\n",
    "def bal_error_rate(v):\n",
    "    perf = performance(v[1], v[0][:,1], bal_err_rate=True)\n",
    "    return perf[\"bal_err_rate\"]\n",
    "\n",
    "# get normal error rate\n",
    "def error_rate(v):\n",
    "    perf = performance(v[1], v[0][:,1], err_rate=True)\n",
    "    return perf[\"err_rate\"]\n",
    "\n",
    "# get auroc\n",
    "def auroc(v):\n",
    "    perf = performance(v[1], v[0][:,1], auroc=True)\n",
    "    return perf[\"auroc\"]\n",
    "\n",
    "def flip_rate(k, v, n):\n",
    "    preds = v[0]\n",
    "    stop = preds.shape[0]\n",
    "    assert stop%n == 0, f\"Number of predictions ({stop}) is not evenly divisible by stepsize ({n})!\"\n",
    "    \n",
    "    noise_seq = False\n",
    "    if \"noise\" in k:\n",
    "        noise_seq = True\n",
    "        \n",
    "    # calculate flip rate for a single image sequence (which contains 'n' number of frames)\n",
    "    n_sum, m = 0, 0\n",
    "    for i in range(0, stop, n):\n",
    "        m += 1\n",
    "        seq_preds = preds[i:i+n].argmax(dim=1)\n",
    "        \n",
    "        if noise_seq:\n",
    "            n_sum += sum(seq_preds[0]!=seq_preds[1:])\n",
    "        else:\n",
    "            n_sum += sum(seq_preds[:-1]!=seq_preds[1:])\n",
    "    \n",
    "    return (n_sum/(m*(n-1))).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def adjust_by_baseline(perf, baseline_model_name:str):\n",
    "    '''Adjust the (corruption) performance of a model by a given baseline'''\n",
    "    \n",
    "    perf_adj = dict()\n",
    "    \n",
    "    for model_name in perf.keys():\n",
    "        perf_adj[model_name] = dict()\n",
    "        \n",
    "        for rep in perf[model_name].keys():\n",
    "            perf_adj[model_name][rep] = dict()\n",
    "            \n",
    "            for tfm_type in perf[model_name][rep].keys():\n",
    "                perf_adj[model_name][rep][tfm_type] = perf[model_name][rep][tfm_type]/perf[baseline_model_name][rep][tfm_type]\n",
    "            \n",
    "    return perf_adj\n",
    "\n",
    "def relative_perf(perf_c, perf_cl):\n",
    "    \n",
    "    def subtract(v, x): return v-x\n",
    "    perf_rel = dict()\n",
    "    \n",
    "    for model_name in perf_c.keys():\n",
    "        perf_rel[model_name] = dict()\n",
    "        \n",
    "        for rep in perf_c[model_name].keys():\n",
    "            apply_subtract = partial(subtract, x=perf_cl[model_name][rep][\"External\"])\n",
    "            perf_rel[model_name][rep] = apply_to_dict_vals(perf_c[model_name][rep], apply_subtract)\n",
    "            \n",
    "    return perf_rel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
