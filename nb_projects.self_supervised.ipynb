{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp projects.self_supervised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-supervised Learning\n",
    "> Self-supervised utility code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "from fastai.vision.all import *\n",
    "from self_supervised.layers import *\n",
    "from self_supervised.models.vision_transformer import *\n",
    "from scp.utils.general import custom_load\n",
    "\n",
    "import torchvision\n",
    "import skimage\n",
    "import colorsys\n",
    "from skimage import io\n",
    "from skimage.measure import find_contours\n",
    "from matplotlib.patches import Polygon\n",
    "import cv2\n",
    "from vit_pytorch.vit import Transformer\n",
    "from einops.layers.torch import Rearrange\n",
    "from einops import repeat\n",
    "from torch import nn\n",
    "from fastai.callback.core import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model splitter functions\n",
    "\n",
    "Split the model architecture at certain points to allow for discriminative learning rates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def model_split1(model):\n",
    "    '''Basic feature extractor & FC head split for '''\n",
    "    groups = L([model.vit_backbone, model.mlp])\n",
    "    return groups.map(params)\n",
    "\n",
    "def model_split2(model):\n",
    "    '''Attempt to do a split like in fastai i.e. feature extractor is split in the middle'''\n",
    "    g1 = nn.Sequential(model.vit_backbone.patch_embed, model.vit_backbone.pos_drop, model.vit_backbone.blocks[:6])\n",
    "    g2 = nn.Sequential(model.vit_backbone.blocks[6:], model.vit_backbone.norm, model.vit_backbone.head)\n",
    "    groups = L([g1, g2, model.mlp])\n",
    "    return groups.map(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "# https://keremturgutlu.github.io/self_supervised/\n",
    "# copy and pasted directly from:\n",
    "# https://github.com/KeremTurgutlu/self_supervised/blob/main/examples/vision/06%20-%20training_dino_iwang.ipynb\n",
    "class ViTClassifier(Module):\n",
    "    def __init__(self, vit_backbone, n_feat_layers, n_classes, lin_f=1024, lin_drop=0.3, pooling='avg'):\n",
    "        self.vit_backbone  = vit_backbone\n",
    "        self.n_feat_layers = n_feat_layers \n",
    "        self.pooling = pooling\n",
    "        out_dim = self.vit_backbone.norm.weight.size(0)\n",
    "        \n",
    "        if self.n_feat_layers == 1: in_f = 2*out_dim\n",
    "        else:\n",
    "            if pooling == 'avg':   in_f = out_dim\n",
    "            elif pooling == 'cat': in_f = out_dim*n_feat_layers\n",
    "        \n",
    "        self.mlp = create_cls_module(in_f, n_classes)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        \n",
    "        out = self.vit_backbone.get_intermediate_layers(x,self.n_feat_layers)\n",
    "        \n",
    "        if self.n_feat_layers == 1:\n",
    "            # cat [CLS] token and avgpooled output tokens from the last layer\n",
    "            cls_token, output_tokens = out[0][:,0],out[0][:,1:]\n",
    "            x = torch.cat([cls_token, output_tokens.mean(1)], dim=1)\n",
    "        else:\n",
    "            # avgpool or cat [CLS] tokens from last n layers\n",
    "            out = [o[:,0] for o in out] \n",
    "            if self.pooling == 'avg':   x = torch.stack(out,dim=0).mean(0)\n",
    "            elif self.pooling == 'cat': x = torch.cat(out, 1)\n",
    "            else:                       raise Exception(\"Pooling should be avg or cat\")\n",
    "                \n",
    "        return self.mlp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def load_state_dict(arch, pretraining, state_dict_path:str=None, **kwargs):\n",
    "    '''Get pretrained arch'''\n",
    "    if arch == \"resnet50\":\n",
    "        if pretraining == \"sl-imagenet\":\n",
    "            return resnet50(pretrained=True).state_dict()\n",
    "        elif pretraining == \"dino-imagenet\":\n",
    "            # https://github.com/facebookresearch/dino\n",
    "            return torch.hub.load('facebookresearch/dino:main', 'dino_resnet50').state_dict()\n",
    "        elif pretraining == \"dino-custom\":\n",
    "            dino_state_dict = custom_load(state_dict_path, **kwargs)\n",
    "            state_dict = OrderedDict()\n",
    "            for k,v in dino_state_dict.items():\n",
    "                if \"teacher\" not in k and k!=\"C\":\n",
    "                    state_dict[k] = v\n",
    "            return state_dict\n",
    "        else:\n",
    "            raise Exception(f\"Unknown pretraining '{arch}'\")\n",
    "            \n",
    "    elif arch == \"deit_small\":\n",
    "        if pretraining == \"sl-imagenet\":\n",
    "            # https://github.com/facebookresearch/deit\n",
    "            return torch.hub.load('facebookresearch/deit:main', 'deit_small_patch16_224', pretrained=True).state_dict()\n",
    "        elif pretraining == \"dino-imagenet\":\n",
    "            # https://github.com/facebookresearch/dino\n",
    "            return torch.hub.load('facebookresearch/dino:main', 'dino_vits16').state_dict()\n",
    "        elif pretraining == \"dino-custom\":\n",
    "            dino_state_dict = custom_load(state_dict_path, **kwargs)\n",
    "            state_dict = OrderedDict()\n",
    "            for k,v in dino_state_dict.items():\n",
    "                if \"teacher\" not in k and k!=\"C\":\n",
    "                    state_dict[k] = v\n",
    "            return state_dict\n",
    "        else:\n",
    "            raise Exception(f\"Unknown pretraining '{arch}'\")\n",
    "            \n",
    "    else:\n",
    "        raise Exception(f\"Unknown architecture '{arch}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def get_dino_arch(arch:str, **kwargs):\n",
    "    '''We do not care about weights here, simply get the model architecture'''\n",
    "\n",
    "    if arch == \"deit_small\": # 21M params\n",
    "        model = deit_small(**kwargs) # naming artefact (dino fastai library and dino FB library)\n",
    "    elif arch == \"resnet50\":\n",
    "        model = resnet50()\n",
    "        model.embed_dim = model.fc.weight.shape[1]\n",
    "        model.fc = Identity()\n",
    "    else:\n",
    "        raise Exception(f\"Unknown architecture '{arch}'\")\n",
    "        \n",
    "    return MultiCropWrapper(model)\n",
    "\n",
    "def get_opt_func(optimizer:str):\n",
    "    '''get optimizer'''\n",
    "    if optimizer==\"adam\":\n",
    "        return Adam\n",
    "    elif optimizer==\"sgd\":\n",
    "        return SGD\n",
    "    else:\n",
    "        raise Exception(f\"No optimizer found that matches '{optimizer}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DINO Visualizations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export \n",
    "# code taken from https://github.com/facebookresearch/dino/blob/main/visualize_attention.py\n",
    "def apply_mask(image, mask, color, alpha=0.5):\n",
    "    for c in range(3):\n",
    "        image[:, :, c] = image[:, :, c] * (1 - alpha * mask) + alpha * mask * color[c] * 255\n",
    "    return image\n",
    "\n",
    "def random_colors(N, bright=True):\n",
    "    \"\"\"\n",
    "    Generate random colors.\n",
    "    \"\"\"\n",
    "    brightness = 1.0 if bright else 0.7\n",
    "    hsv = [(i / N, 1, brightness) for i in range(N)]\n",
    "    colors = list(map(lambda c: colorsys.hsv_to_rgb(*c), hsv))\n",
    "    random.shuffle(colors)\n",
    "    return colors\n",
    "\n",
    "def display_instances(image, mask, fname=\"test\", figsize=(5, 5), blur=False, contour=True, alpha=0.5):\n",
    "    fig = plt.figure(figsize=figsize, frameon=False)\n",
    "    ax = plt.Axes(fig, [0., 0., 1., 1.])\n",
    "    ax.set_axis_off()\n",
    "    fig.add_axes(ax)\n",
    "    ax = plt.gca()\n",
    "\n",
    "    N = 1\n",
    "    mask = mask[None, :, :]\n",
    "    # Generate random colors\n",
    "    colors = random_colors(N)\n",
    "\n",
    "    # Show area outside image boundaries.\n",
    "    height, width = image.shape[:2]\n",
    "    margin = 0\n",
    "    ax.set_ylim(height + margin, -margin)\n",
    "    ax.set_xlim(-margin, width + margin)\n",
    "    ax.axis('off')\n",
    "    masked_image = image.astype(np.uint32).copy()\n",
    "    for i in range(N):\n",
    "        color = colors[i]\n",
    "        _mask = mask[i]\n",
    "        if blur:\n",
    "            _mask = cv2.blur(_mask,(10,10))\n",
    "        # Mask\n",
    "        masked_image = apply_mask(masked_image, _mask, color, alpha)\n",
    "        # Mask Polygon\n",
    "        # Pad to ensure proper polygons for masks that touch image edges.\n",
    "        if contour:\n",
    "            padded_mask = np.zeros((_mask.shape[0] + 2, _mask.shape[1] + 2))\n",
    "            padded_mask[1:-1, 1:-1] = _mask\n",
    "            contours = find_contours(padded_mask, 0.5)\n",
    "            for verts in contours:\n",
    "                # Subtract the padding and flip (y, x) to (x, y)\n",
    "                verts = np.fliplr(verts) - 1\n",
    "                p = Polygon(verts, facecolor=\"none\", edgecolor=color)\n",
    "                ax.add_patch(p)\n",
    "    ax.imshow(masked_image.astype(np.uint8), aspect='auto')\n",
    "    fig.savefig(fname)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export \n",
    "\n",
    "\n",
    "class MAECustom(nn.Module):\n",
    "    '''Adapted MAE implementation from https://github.com/lucidrains/vit-pytorch implementation\n",
    "    \n",
    "    This MAE can be initialized with Timm and FB ViTs, thus taking advantage of pretraining.\n",
    "    The loss is also not calculated in the forward() function anymore. \n",
    "    Insted the preds and ground truths are returned (instead of the loss)\n",
    "    '''\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        encoder,\n",
    "        decoder_dim,\n",
    "        masking_ratio = 0.75,\n",
    "        decoder_depth = 1,\n",
    "        decoder_heads = 8,\n",
    "        decoder_dim_head = 64\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert masking_ratio > 0 and masking_ratio < 1, 'masking ratio must be kept between 0 and 1'\n",
    "        self.masking_ratio = masking_ratio\n",
    "\n",
    "        # extract some hyperparameters and functions from encoder (vision transformer to be trained)\n",
    "\n",
    "        self.encoder = encoder\n",
    "        \n",
    "        channel_num = self.encoder.patch_embed.proj.weight.shape[1]\n",
    "        \n",
    "        num_patches, encoder_dim = self.encoder.patch_embed.num_patches+1, encoder.embed_dim      \n",
    "        \n",
    "        # recreate 'to_patch' function from https://github.com/lucidrains/vit-pytorch implementation\n",
    "        if isinstance(self.encoder.patch_embed.patch_size, int):\n",
    "            self.encoder.patch_embed.patch_size = (self.encoder.patch_embed.patch_size, self.encoder.patch_embed.patch_size)\n",
    "        patch_height, patch_width = self.encoder.patch_embed.patch_size\n",
    "        self.to_patch =  Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_height, p2 = patch_width)\n",
    "        pixel_values_per_patch = self.encoder.patch_embed.patch_size[0]**2 * channel_num\n",
    "    \n",
    "        # decoder parameters\n",
    "\n",
    "        self.enc_to_dec = nn.Linear(encoder_dim, decoder_dim) if encoder_dim != decoder_dim else nn.Identity()\n",
    "        self.mask_token = nn.Parameter(torch.randn(decoder_dim))\n",
    "        self.decoder = Transformer(dim = decoder_dim, depth = decoder_depth, heads = decoder_heads, dim_head = decoder_dim_head, mlp_dim = decoder_dim * 4)\n",
    "        self.decoder_pos_emb = nn.Embedding(num_patches, decoder_dim)\n",
    "        self.to_pixels = nn.Linear(decoder_dim, pixel_values_per_patch)\n",
    "\n",
    "    def forward(self, img):\n",
    "        device = img.device\n",
    "        \n",
    "        # get patches\n",
    "        \n",
    "        patches = self.to_patch(img)\n",
    "        batch, num_patches, *_ = patches.shape\n",
    "\n",
    "        # img to encoder tokens and add positions\n",
    "\n",
    "        tokens = self.encoder.patch_embed(img)\n",
    "        tokens = tokens + self.encoder.pos_embed[:, 1:(num_patches + 1)]\n",
    "        \n",
    "        # calculate of patches needed to be masked, and get random indices, dividing it up for mask vs unmasked\n",
    "\n",
    "        num_masked = int(self.masking_ratio * num_patches)\n",
    "        rand_indices = torch.rand(batch, num_patches, device = device).argsort(dim = -1)\n",
    "        masked_indices, unmasked_indices = rand_indices[:, :num_masked], rand_indices[:, num_masked:]\n",
    "\n",
    "        # get the unmasked tokens to be encoded\n",
    "\n",
    "        batch_range = torch.arange(batch, device = device)[:, None]\n",
    "        tokens = tokens[batch_range, unmasked_indices]\n",
    "\n",
    "        # get the patches to be masked for the final reconstruction loss\n",
    "\n",
    "        masked_patches = patches[batch_range, masked_indices]\n",
    "\n",
    "        # attend with vision transformer\n",
    "\n",
    "        encoded_tokens = self.encoder.blocks(tokens)\n",
    "\n",
    "        # project encoder to decoder dimensions, if they are not equal - the paper says you can get away with a smaller dimension for decoder\n",
    "\n",
    "        decoder_tokens = self.enc_to_dec(encoded_tokens)\n",
    "\n",
    "        # repeat mask tokens for number of masked, and add the positions using the masked indices derived above\n",
    "\n",
    "        mask_tokens = repeat(self.mask_token, 'd -> b n d', b = batch, n = num_masked)\n",
    "        mask_tokens = mask_tokens + self.decoder_pos_emb(masked_indices)\n",
    "\n",
    "        # concat the masked tokens to the decoder tokens and attend with decoder\n",
    "\n",
    "        decoder_tokens = torch.cat((mask_tokens, decoder_tokens), dim = 1)\n",
    "        decoded_tokens = self.decoder(decoder_tokens)\n",
    "\n",
    "        # splice out the mask tokens and project to pixel values\n",
    "\n",
    "        mask_tokens = decoded_tokens[:, :num_masked]\n",
    "        pred_pixel_values = self.to_pixels(mask_tokens)\n",
    "\n",
    "        # calculate reconstruction loss\n",
    "        return (pred_pixel_values, masked_patches)\n",
    "    \n",
    "class MAECallback(Callback):\n",
    "    '''Make MAE output compatible to fastai's training loop\n",
    "    \n",
    "    Semi-official MAE implementation returns not just predictions but also the ground truth. \n",
    "    Fastai does not expect this, therefore this needs to be split before loss is calculated. \n",
    "    '''\n",
    "    \n",
    "    def after_pred(self):\n",
    "        '''Split MAE model output into predicition and ground truth'''\n",
    "        self.learn.yb = (self.learn.pred[1],) # yb only supports tuple assignement\n",
    "        self.learn.pred = self.learn.pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
