{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp train.discriminative_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discriminative lr\n",
    "\n",
    "> Discriminative lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export \n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import functools\n",
    "import torch\n",
    "from typing import Union\n",
    "\n",
    "'''\n",
    "Developped by the Fastai team for the Fastai library\n",
    "From the fastai library\n",
    "https://www.fast.ai and https://github.com/fastai/fastai\n",
    "\n",
    "Modified by vdouet for pytorch and copied from https://github.com/vdouet/Discriminative-learning-rates-PyTorch\n",
    "'''\n",
    "\n",
    "###############################################################################\n",
    "#Unmodified classes and functions:\n",
    "\n",
    "class PrePostInitMeta(type):\n",
    "    \"A metaclass that calls optional `__pre_init__` and `__post_init__` methods\"\n",
    "    def __new__(cls, name, bases, dct):\n",
    "        x = super().__new__(cls, name, bases, dct)\n",
    "        old_init = x.__init__\n",
    "        def _pass(self): pass\n",
    "        @functools.wraps(old_init)\n",
    "        def _init(self,*args,**kwargs):\n",
    "            self.__pre_init__()\n",
    "            old_init(self, *args,**kwargs)\n",
    "            self.__post_init__()\n",
    "        x.__init__ = _init\n",
    "        if not hasattr(x,'__pre_init__'):  x.__pre_init__  = _pass\n",
    "        if not hasattr(x,'__post_init__'): x.__post_init__ = _pass\n",
    "        return x\n",
    "\n",
    "class Module(nn.Module, metaclass=PrePostInitMeta):\n",
    "    \"Same as `nn.Module`, but no need for subclasses to call `super().__init__`\"\n",
    "    def __pre_init__(self): super().__init__()\n",
    "    def __init__(self): pass\n",
    "\n",
    "class ParameterModule(Module):\n",
    "    \"Register a lone parameter `p` in a module.\"\n",
    "    def __init__(self, p:nn.Parameter): self.val = p\n",
    "    def forward(self, x): return x\n",
    "\n",
    "def children(m:nn.Module):\n",
    "    \"Get children of `m`.\"\n",
    "    return list(m.children())\n",
    "\n",
    "def num_children(m:nn.Module):\n",
    "    \"Get number of children modules in `m`.\"\n",
    "    return len(children(m))\n",
    "\n",
    "def children_and_parameters(m:nn.Module):\n",
    "    \"Return the children of `m` and its direct parameters not registered in modules.\"\n",
    "    children = list(m.children())\n",
    "    children_p = sum([[id(p) for p in c.parameters()] for c in m.children()],[])\n",
    "    for p in m.parameters():\n",
    "        if id(p) not in children_p: children.append(ParameterModule(p))\n",
    "    return children\n",
    "\n",
    "def even_mults(start:float, stop:float, n:int)->np.ndarray:\n",
    "    \"Build log-stepped array from `start` to `stop` in `n` steps.\"\n",
    "    mult = stop/start\n",
    "    step = mult**(1/(n-1))\n",
    "    return np.array([start*(step**i) for i in range(n)])\n",
    "\n",
    "flatten_model = lambda m: sum(map(flatten_model,children_and_parameters(m)),[]) if num_children(m) else [m]\n",
    "###############################################################################\n",
    "\n",
    "'''\n",
    "Modified version of lr_range from fastai\n",
    "https://github.com/fastai/fastai/blob/master/fastai/basic_train.py#L185\n",
    "'''\n",
    "def lr_range(net:nn.Module, lr:slice, model_len:int)->np.ndarray:\n",
    "        \"Build differential learning rates from `lr`.\"\n",
    "\n",
    "        if not isinstance(lr,slice): return lr\n",
    "        if lr.start: res = even_mults(lr.start, lr.stop, model_len)\n",
    "        else: res = [lr.stop/10]*(model_len-1) + [lr.stop]\n",
    "        return res\n",
    "\n",
    "def unfreeze_layers(model:nn.Sequential, unfreeze:bool=True)->None:\n",
    "    \"Unfreeze or freeze all layers\"\n",
    "\n",
    "    for layer in model.parameters():\n",
    "        layer.requires_grad = unfreeze\n",
    "\n",
    "def build_param_dicts(layers:nn.Sequential, lr:list=[0], return_len:bool=False)->Union[int,list]:\n",
    "    '''\n",
    "    Either return the number of layers with requires_grad is True\n",
    "    or return a list of dictionnaries containing each layers on its associated LR\"\n",
    "    Both weight and bias are check for requires_grad is True\n",
    "    '''\n",
    "\n",
    "    params = []\n",
    "    idx = 0\n",
    "    for layer in layers:\n",
    "        param = []\n",
    "        if (hasattr(layer, \"requires_grad\") and layer.requires_grad):\n",
    "            #To implement for custom nn.Parameter()\n",
    "            print(\"Custom nn.Parameter() not supported\")\n",
    "        if(hasattr(layer, \"weight\") and layer.weight.requires_grad):\n",
    "            param.append(layer.weight)\n",
    "        if (hasattr(layer, \"bias\") and hasattr(layer.bias, \"requires_grad\") and layer.bias.requires_grad):\n",
    "            param.append(layer.bias)\n",
    "        \n",
    "        # only modification from vdouet's code. Changed lr from string to number\n",
    "        # if param: params.append({'params': param, 'lr': f'{lr[idx]}'}); idx += 1\n",
    "        if param: params.append({'params': param, 'lr': lr[idx]}); idx += 1\n",
    "        if return_len: idx = 0 #We don't want to increment idx here.\n",
    "\n",
    "    return len(params) if return_len else params\n",
    "\n",
    "def discriminative_lr_params(net:nn.Module, lr:slice, unfreeze:bool=True)->Union[list,np.ndarray,nn.Sequential]:\n",
    "    '''\n",
    "    Flatten our model and generate a list of dictionnaries to be passed to the\n",
    "    optimizer.\n",
    "    - If only one learning rate is passed as a slice the last layer will have the\n",
    "    corresponding learning rate and all other ones will have lr/10\n",
    "    - If two learning rates are passed such as slice(min_lr, max_lr) the last\n",
    "    layer will have max_lr as a learning rate and the first one will have min_lr.\n",
    "    All middle layers will have learning rates logarithmically interpolated\n",
    "    ranging from min_lr to max_lr\n",
    "    '''\n",
    "\n",
    "    layers = nn.Sequential(*flatten_model(net)) #Flatten/ungroup our model\n",
    "    if unfreeze: unfreeze_layers(layers, True)  #Unfreeze all layers\n",
    "\n",
    "    #Return the number of layer where requires_grad is True (bias + weight)\n",
    "    model_len = build_param_dicts(layers, return_len=True)\n",
    "\n",
    "    #Create the list of learning rates\n",
    "    list_lr = lr_range(net, lr, model_len)\n",
    "\n",
    "    #Create our optimizer parameters list of dictionnaries\n",
    "    params_layers = build_param_dicts(layers, list_lr)\n",
    "\n",
    "    return params_layers, np.array(list_lr), layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
