{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp train.general"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General training methods\n",
    "\n",
    "> Various general methods to be used during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from fastai.learner import Recorder\n",
    "from fastprogress.fastprogress import format_time\n",
    "from csv import writer\n",
    "import time\n",
    "#from fastai.callback.tensorboard import TensorBoardCallback, tensorboard_log\n",
    "from fastcore.imports import *\n",
    "from fastai.callback.tracker import ReduceLROnPlateau, TrackerCallback\n",
    "from fastcore.basics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def pp_dls(dl, show_tfms:bool=False):\n",
    "    '''Pretty print dl contents'''\n",
    "\n",
    "    # each dataloader has datasets (not dataset, note the plural)\n",
    "    # I think it is usally two, one for x (image) and one for y (label) with much overlap between the two\n",
    "    dsx = dl.dataset.tls[0]\n",
    "    dsy = dl.dataset.tls[1]\n",
    "\n",
    "    # get df for each (obviously requires that a df was used to define the datablock)\n",
    "    dss_df = dsx.items # (could also be dxy, does not matter)\n",
    "\n",
    "    # get x and y labels \n",
    "    x_label = dsx.tfms.col_reader.cols.items[0]\n",
    "    y_label = dsy.tfms.col_reader.cols.items[0]\n",
    "\n",
    "    # pp\n",
    "    print(f\"No. Images:\\t {dl.n}\")\n",
    "    print(f\"Classes:\\t {dss_df[y_label].unique()}\")\n",
    "    print(f\"Shuffle:\\t {dl.shuffle}\")\n",
    "    print(f\"Bs:\\t\\t {dl.bs}\\n\")\n",
    "    if show_tfms:\n",
    "        print(\"Transformations\\n\")\n",
    "        for i, tfm in enumerate(dl.after_batch.fs, 1):\n",
    "            print(f\"\\t{i}.) {tfm.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export \n",
    "class SilentRecorder(Recorder):\n",
    "\n",
    "    def __init__(self, csv_file, add_time=True, train_metrics=False, valid_metrics=True, beta=0.98):\n",
    "        super().__init__(add_time, train_metrics, valid_metrics, beta)\n",
    "        self.csv_file = f\"{csv_file}.csv\"\n",
    "        open(self.csv_file, \"w\").close()\n",
    "        \n",
    "    def before_fit(self):\n",
    "        super().before_fit()\n",
    "        self.append_to_csv(self.metric_names)\n",
    "        \n",
    "    def after_epoch(self):\n",
    "        \"Store and log the loss/metric values without verbose printing\"\n",
    "        self.learn.final_record = self.log[1:].copy()\n",
    "        self.values.append(self.learn.final_record)\n",
    "        if self.add_time: self.log.append(format_time(time.time() - self.start_epoch))\n",
    "        self.iters.append(self.smooth_loss.count)\n",
    "        self.append_to_csv(self.log)\n",
    "\n",
    "    def after_fit(self):\n",
    "        self.append_to_csv([])\n",
    "        \n",
    "    def append_to_csv(self, row):\n",
    "        with open(self.csv_file, 'a') as f:\n",
    "            writer_object = writer(f)\n",
    "            writer_object.writerow(row)\n",
    "            f.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "# class MultiFitTensorBoardCallback(TensorBoardCallback):\n",
    "#     \"Saves model topology, losses & metrics for tensorboard and tensorboard projector during training\"\n",
    "#     def __init__(self, log_dir=None, trace_model=True, log_preds=True, n_preds=9, projector=False, layer=None):\n",
    "#         super().__init__(log_dir, trace_model, log_preds, n_preds, projector, layer)\n",
    "#         self.num_fit = -1\n",
    "#         self.custom_iter = 0\n",
    "        \n",
    "#     def before_fit(self):\n",
    "#         super().before_fit()\n",
    "#         self.learn.train_iter = self.custom_iter\n",
    "#         self.num_fit += 1\n",
    "\n",
    "#     def after_batch(self):\n",
    "#         self.writer.add_scalar('train_loss', self.smooth_loss, (self.num_fit*self.custom_iter)+self.train_iter)\n",
    "#         for i,h in enumerate(self.opt.hypers):\n",
    "#             for k,v in h.items(): self.writer.add_scalar(f'{k}_{i}', v, (self.num_fit*self.custom_iter)+self.train_iter)\n",
    "\n",
    "#     def after_epoch(self):\n",
    "#         for n,v in zip(self.recorder.metric_names[2:-1], self.recorder.log[2:-1]):\n",
    "#             self.writer.add_scalar(n, v, (self.num_fit*self.custom_iter)+self.train_iter)\n",
    "#         if self.log_preds:\n",
    "#             b = self.dls.valid.one_batch()\n",
    "#             self.learn.one_batch(0, b)\n",
    "#             preds = getattr(self.loss_func, 'activation', noop)(self.pred)\n",
    "#             out = getattr(self.loss_func, 'decodes', noop)(preds)\n",
    "#             x,y,its,outs = self.dls.valid.show_results(b, out, show=False, max_n=self.n_preds)\n",
    "#             tensorboard_log(x, y, its, outs, self.writer, (self.num_fit*self.custom_iter)+self.train_iter)\n",
    "            \n",
    "#     def after_fit(self):\n",
    "#         self.custom_iter = self.learn.train_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class ReduceLROnPlateauNTimes(ReduceLROnPlateau):\n",
    "    def __init__(self, n, monitor='valid_loss', comp=None, min_delta=0., patience=1, factor=10., min_lr=0, reset_on_fit=True):\n",
    "        super().__init__( monitor=monitor, comp=comp, min_delta=min_delta, patience=patience, factor=factor, min_lr=min_lr, reset_on_fit=reset_on_fit)\n",
    "        self.n = n\n",
    "        self.counter = 0\n",
    "        self.alive = True\n",
    "        \n",
    "    def after_epoch(self):\n",
    "        \"Compare the value monitored to its best score and reduce LR by `factor` if no improvement.\"\n",
    "        TrackerCallback.after_epoch(self)\n",
    "        if self.new_best: self.wait = 0\n",
    "        elif self.counter < self.n:\n",
    "            self.wait += 1\n",
    "            if self.wait >= self.patience:\n",
    "                self.counter += 1\n",
    "                old_lr = self.opt.hypers[-1]['lr']\n",
    "                for h in self.opt.hypers: h['lr'] = max(h['lr'] / self.factor, self.min_lr)\n",
    "                self.wait = 0\n",
    "                if self.opt.hypers[-1][\"lr\"] < old_lr:\n",
    "                    print(f'Epoch {self.epoch}: reducing lr to {self.opt.hypers[-1][\"lr\"]}')\n",
    "        else:\n",
    "            if self.alive:\n",
    "                print(f\"Epoch {self.epoch}: Killing callback '{self.__class__.__name__}' as already reduced {self.counter} time(s)\")\n",
    "                self.alive = False"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
